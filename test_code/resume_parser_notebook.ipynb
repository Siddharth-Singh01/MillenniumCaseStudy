{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Parser - LangGraph Workflow\n",
    "\n",
    "Simple notebook to parse resumes using LangGraph workflow.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup: Initialize parser\n",
    "2. Input: Provide document path\n",
    "3. Process: Run LangGraph extraction\n",
    "4. Output: View structured results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grab's mission is to drive Southeast Asia forward by creating economic empowerment for everyone through safe, accessible, and affordable transportation, delivery, and financial services.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "model = ChatOpenAI(\n",
    "    model=\"aws/global.anthropic.claude-sonnet-4-5-20250929-v1:0\",  # replace with your model here\n",
    "    openai_api_key= '', # Reminder: Never hard-code key in code! Access securely through environment variable or vault.\n",
    "    openai_api_base=\"\"  # replace with the proper endpoint here\n",
    ")\n",
    "print(model.invoke([HumanMessage(content=\"Describe SEA company Grab's mission in one sentence.\")]).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'John Doe'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicInfoSchema(name='John Doe', email=None, location=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicInfoSchema(**result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: john@example.com\n",
      "Location: Singapore\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class BasicInfoSchema(BaseModel):\n",
    "    name: str = Field(description=\"Full name - always extract\")\n",
    "    email: str = Field(default=\"\", description=\"Email address - extract if present, use empty string if not found\")\n",
    "    location: str = Field(default=\"\", description=\"Location - extract if present, use empty string if not found\")\n",
    "    \n",
    "    def get_email(self) -> Optional[str]:\n",
    "        \"\"\"Helper to get email as Optional[str]\"\"\"\n",
    "        return self.email if self.email else None\n",
    "    \n",
    "    def get_location(self) -> Optional[str]:\n",
    "        \"\"\"Helper to get location as Optional[str]\"\"\"\n",
    "        return self.location if self.location else None\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"aws/global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    openai_api_key='',\n",
    "    openai_api_base=\""\n",
    ")\n",
    "\n",
    "structured_model = model.with_structured_output(BasicInfoSchema)\n",
    "result = structured_model.invoke([HumanMessage(\n",
    "    content=\"Extract info: Name: John Doe, Email: john@example.com, Location: Singapore\"\n",
    ")])\n",
    "result = BasicInfoSchema(**result)\n",
    "print(\"Email:\", result.get_email())  # \"john@example.com\"\n",
    "print(\"Location:\", result.get_location())  # \"Singapore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'get_email'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     25\u001b[39m structured_model = model.with_structured_output(BasicInfoSchema)\n\u001b[32m     26\u001b[39m result = structured_model.invoke([HumanMessage(\n\u001b[32m     27\u001b[39m     content=\u001b[33m\"\u001b[39m\u001b[33mExtract info: Name: John Doe, Email: john@example.com, Location: Singapore\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m )])\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmail:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_email\u001b[49m())  \u001b[38;5;66;03m# \"john@example.com\"\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLocation:\u001b[39m\u001b[33m\"\u001b[39m, result.get_location())  \u001b[38;5;66;03m# \"Singapore\"\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'get_email'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# Simple test schema\n",
    "class BasicInfoSchema(BaseModel):\n",
    "    name: str = Field(description=\"Full name\")\n",
    "    email: str = Field(default=None, description=\"Email address\")\n",
    "    location: str = Field(default=None, description=\"Location\")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"aws/global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    openai_api_key='',\n",
    "    openai_api_base=\""\n",
    ")\n",
    "\n",
    "# Use with_structured_output\n",
    "structured_model = model.with_structured_output(BasicInfoSchema)\n",
    "\n",
    "# More explicit prompt - instruct to extract ALL fields\n",
    "prompt = \"\"\"Extract the following information from the text below. Extract ALL available fields.\n",
    "\n",
    "Extract:\n",
    "- Full name (REQUIRED)\n",
    "- Email address (if present)\n",
    "- Location (if present)\n",
    "\n",
    "Text to extract from:\n",
    "Name: John Doe, Email: john@example.com, Location: Singapore\n",
    "\"\"\"\n",
    "\n",
    "result = structured_model.invoke([HumanMessage(content=prompt)])\n",
    "print(\"Result dict:\", result)\n",
    "print(\"Schema instance:\", BasicInfoSchema(**result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_settings import BaseSettings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: âœ… SET\n",
      "Base URL: \n",
      "âœ… Parser initialized and ready!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import and initialize parser\n",
    "from resume_parser_langchain import ResumeParser\n",
    "from config import get_config\n",
    "\n",
    "# Initialize configuration\n",
    "config = get_config()\n",
    "\n",
    "# Verify config loaded correctly\n",
    "llm_config = config.get_llm_config()\n",
    "print(f\"API Key: {'SET' if llm_config.api_key else 'NOT SET'}\")\n",
    "print(f\"Base URL: {'SET' if llm_config.base_url else 'NOT SET'}\")\n",
    "\n",
    "# Initialize parser (this sets up LangGraph workflow)\n",
    "parser = ResumeParser(config=config)\n",
    "\n",
    "print(\"âœ… Parser initialized and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e67aabc3-4d2c-47b7-83bd-b3fd4ccc40be'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Parser initialized and ready!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import and initialize parser\n",
    "from resume_parser_langchain import ResumeParser\n",
    "from config import get_config\n",
    "\n",
    "# Initialize configuration\n",
    "config = get_config()\n",
    "\n",
    "# Initialize parser (this sets up LangGraph workflow)\n",
    "parser = ResumeParser(config=config)\n",
    "\n",
    "print(\"âœ… Parser initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input: Provide Document Path\n",
    "\n",
    "Change `doc_name` below to your resume file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Document: /Users/siddharth.singh/Downloads/2025 DS Case Study/input/Chen Li (Alex).docx\n"
     ]
    }
   ],
   "source": [
    "# Input: Change this to your document path\n",
    "doc_name = \"/Users/siddharth.singh/Downloads/2025 DS Case Study/input/Chen Li (Alex).docx\"  # Update this path\n",
    "\n",
    "print(f\"ðŸ“„ Document: {doc_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process: Run LangGraph Workflow\n",
    "\n",
    "This runs the complete workflow:\n",
    "1. **Guardrail**: Validates document is a resume\n",
    "2. **Text Extraction**: Extracts text using vision loader\n",
    "3. **LangGraph Extraction**: Multi-step structured data extraction\n",
    "4. **Returns**: Structured JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m parser.parse_resume(file_path: str) -> Dict[str, Any]\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Parse resume using GPT-4o vision API\n",
      "\n",
      "Process:\n",
      "Step 0: Validate document is a resume (guardrail)\n",
      "Step 1: Load document with GPT-4o vision loader\n",
      "Step 2: Split into chunks\n",
      "Step 3: Extract structured data with LLM (multi-step LangGraph workflow)\n",
      "Step 4: Generate UUIDs for chunks\n",
      "Step 5: Enrich with calculated fields\n",
      "\n",
      "Args:\n",
      "    file_path: Path to resume file (PDF or DOCX)\n",
      "    \n",
      "Returns:\n",
      "    Dictionary containing:\n",
      "    - resume_uuid: Unique identifier for resume\n",
      "    - file_path: Original file path\n",
      "    - file_name: Original file name\n",
      "    - structured_data: Extracted structured data (ResumeSchema)\n",
      "    - full_text: Combined text from all documents\n",
      "    - chunks: List of document chunks with UUIDs\n",
      "    - extraction_timestamp: When extraction occurred\n",
      "    - validation_result: Document validation result (if validation enabled)\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m parse_resume(self, file_path: str) -> Dict[str, Any]:\n",
      "        \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m        Parse resume using GPT-4o vision API\u001b[39m\n",
      "\u001b[33m        \u001b[39m\n",
      "\u001b[33m        Process:\u001b[39m\n",
      "\u001b[33m        Step 0: Validate document is a resume (guardrail)\u001b[39m\n",
      "\u001b[33m        Step 1: Load document with GPT-4o vision loader\u001b[39m\n",
      "\u001b[33m        Step 2: Split into chunks\u001b[39m\n",
      "\u001b[33m        Step 3: Extract structured data with LLM (multi-step LangGraph workflow)\u001b[39m\n",
      "\u001b[33m        Step 4: Generate UUIDs for chunks\u001b[39m\n",
      "\u001b[33m        Step 5: Enrich with calculated fields\u001b[39m\n",
      "\u001b[33m        \u001b[39m\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            file_path: Path to resume file (PDF or DOCX)\u001b[39m\n",
      "\u001b[33m            \u001b[39m\n",
      "\u001b[33m        Returns:\u001b[39m\n",
      "\u001b[33m            Dictionary containing:\u001b[39m\n",
      "\u001b[33m            - resume_uuid: Unique identifier for resume\u001b[39m\n",
      "\u001b[33m            - file_path: Original file path\u001b[39m\n",
      "\u001b[33m            - file_name: Original file name\u001b[39m\n",
      "\u001b[33m            - structured_data: Extracted structured data (ResumeSchema)\u001b[39m\n",
      "\u001b[33m            - full_text: Combined text from all documents\u001b[39m\n",
      "\u001b[33m            - chunks: List of document chunks with UUIDs\u001b[39m\n",
      "\u001b[33m            - extraction_timestamp: When extraction occurred\u001b[39m\n",
      "\u001b[33m            - validation_result: Document validation result (if validation enabled)\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        file_path = Path(file_path)\n",
      "        \n",
      "        \u001b[38;5;66;03m# Step 0: Validate document is a resume (guardrail)\u001b[39;00m\n",
      "        validation_result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.validator:\n",
      "            validation_result = self.validator.validate_resume_preview(file_path)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m validation_result.is_resume:\n",
      "                \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\n",
      "                    \u001b[33mf\"Document validation failed: {validation_result.reason}. \"\u001b[39m\n",
      "                    \u001b[33mf\"Confidence: {validation_result.confidence:.2f}. \"\u001b[39m\n",
      "                    \u001b[33mf\"Document type detected: {validation_result.document_type or 'unknown'}. \"\u001b[39m\n",
      "                    \u001b[33mf\"Please upload a valid resume/CV document.\"\u001b[39m\n",
      "                )\n",
      "        \n",
      "        \u001b[38;5;66;03m# Generate resume UUID\u001b[39;00m\n",
      "        resume_uuid = self.uuid_generator.generate_resume_uuid(file_path.name)\n",
      "        \n",
      "        \u001b[38;5;66;03m# Step 1: Load document with GPT-4o vision\u001b[39;00m\n",
      "        documents = self.vision_loader.load_document(file_path)\n",
      "        \n",
      "        \u001b[38;5;66;03m# Step 2: Split into chunks\u001b[39;00m\n",
      "        chunks = self.text_splitter.split_documents(documents)\n",
      "        \n",
      "        \u001b[38;5;66;03m# Combine full text\u001b[39;00m\n",
      "        full_text = \u001b[33m\"\\n\\n\"\u001b[39m.join([doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;28;01min\u001b[39;00m documents])\n",
      "        \n",
      "        \u001b[38;5;66;03m# Validate full_text extraction\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m full_text \u001b[38;5;28;01mor\u001b[39;00m len(full_text.strip()) < \u001b[32m50\u001b[39m:\n",
      "            print(\u001b[33mf\"Text extraction seems to have failed. Full text length: {len(full_text)}\"\u001b[39m)\n",
      "            print(\u001b[33mf\"Document pages: {len(documents)}\"\u001b[39m)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m documents:\n",
      "                print(\u001b[33mf\"First page content length: {len(documents[0].page_content) if documents[0].page_content else 0}\"\u001b[39m)\n",
      "        \n",
      "        \u001b[38;5;66;03m# Step 3: Extract structured data using multi-step LangGraph workflow\u001b[39;00m\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# Initialize extraction state (TypedDict behaves like a dict)\u001b[39;00m\n",
      "            initial_state: ResumeExtractionState = {\n",
      "                \u001b[33m'full_text'\u001b[39m: full_text,\n",
      "                \u001b[33m'extracted_data'\u001b[39m: {},\n",
      "                \u001b[33m'task_log'\u001b[39m: [],\n",
      "                \u001b[33m'error'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                \u001b[33m'resume_uuid'\u001b[39m: resume_uuid,\n",
      "                \u001b[33m'file_path'\u001b[39m: str(file_path)\n",
      "            }\n",
      "            \n",
      "            \u001b[38;5;66;03m# Build and execute extraction graph\u001b[39;00m\n",
      "            extraction_graph = self._build_extraction_graph()\n",
      "            result_state = extraction_graph.invoke(initial_state)\n",
      "            \n",
      "            \u001b[38;5;66;03m# Extract structured data from result state (LangGraph returns dict)\u001b[39;00m\n",
      "            extracted_data = result_state.get(\u001b[33m'extracted_data'\u001b[39m, {})\n",
      "            \n",
      "            \u001b[38;5;66;03m# Create ResumeSchema from extracted data\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# Provide default for name if missing (only required field)\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'name'\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m extracted_data \u001b[38;5;28;01mor\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m extracted_data.get(\u001b[33m'name'\u001b[39m):\n",
      "                extracted_data[\u001b[33m'name'\u001b[39m] = \u001b[33m'Unknown'\u001b[39m\n",
      "            \n",
      "            structured_data = ResumeSchema(**extracted_data)\n",
      "            \n",
      "            \u001b[38;5;66;03m# Log extraction steps for debugging\u001b[39;00m\n",
      "            task_log = result_state.get(\u001b[33m'task_log'\u001b[39m, [])\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m task_log:\n",
      "                print(\u001b[33mf\"Extraction steps completed: {', '.join(task_log)}\"\u001b[39m)\n",
      "            \n",
      "            \u001b[38;5;66;03m# Log any errors but continue with partial extraction\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m result_state.get(\u001b[33m'error'\u001b[39m):\n",
      "                print(\u001b[33mf\"Extraction warnings: {result_state['error']}\"\u001b[39m)\n",
      "                \n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            \u001b[38;5;28;01mimport\u001b[39;00m traceback\n",
      "            traceback.print_exc()\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33mf\"Resume extraction failed: {e}\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m e\n",
      "        \n",
      "        \u001b[38;5;66;03m# Step 4: Generate UUIDs for chunks\u001b[39;00m\n",
      "        chunked_docs = []\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m idx, chunk \u001b[38;5;28;01min\u001b[39;00m enumerate(chunks):\n",
      "            \u001b[38;5;66;03m# Determine section from metadata or content\u001b[39;00m\n",
      "            section = self._determine_section(chunk)\n",
      "            \n",
      "            chunk_uuid = self.uuid_generator.generate_chunk_uuid(\n",
      "                resume_uuid=resume_uuid,\n",
      "                section=section,\n",
      "                index=idx\n",
      "            )\n",
      "            \n",
      "            chunked_docs.append({\n",
      "                \u001b[33m'chunk_uuid'\u001b[39m: chunk_uuid,\n",
      "                \u001b[33m'resume_uuid'\u001b[39m: resume_uuid,\n",
      "                \u001b[33m'content'\u001b[39m: chunk.page_content,\n",
      "                \u001b[33m'metadata'\u001b[39m: chunk.metadata,\n",
      "                \u001b[33m'section'\u001b[39m: section\n",
      "            })\n",
      "        \n",
      "        \u001b[38;5;66;03m# Step 5: Enrich with calculated fields\u001b[39;00m\n",
      "        structured_data = self._enrich_data(structured_data)\n",
      "        \n",
      "        result = {\n",
      "            \u001b[33m'resume_uuid'\u001b[39m: resume_uuid,\n",
      "            \u001b[33m'file_path'\u001b[39m: str(file_path),\n",
      "            \u001b[33m'file_name'\u001b[39m: file_path.name,\n",
      "            \u001b[33m'structured_data'\u001b[39m: structured_data,\n",
      "            \u001b[33m'full_text'\u001b[39m: full_text,\n",
      "            \u001b[33m'chunks'\u001b[39m: chunked_docs,\n",
      "            \u001b[33m'extraction_timestamp'\u001b[39m: datetime.now().isoformat()\n",
      "        }\n",
      "        \n",
      "        \u001b[38;5;66;03m# Add validation result if available\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m validation_result:\n",
      "            result[\u001b[33m'validation_result'\u001b[39m] = {\n",
      "                \u001b[33m'is_resume'\u001b[39m: validation_result.is_resume,\n",
      "                \u001b[33m'confidence'\u001b[39m: validation_result.confidence,\n",
      "                \u001b[33m'reason'\u001b[39m: validation_result.reason,\n",
      "                \u001b[33m'document_type'\u001b[39m: validation_result.document_type\n",
      "            }\n",
      "        \n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mFile:\u001b[39m      ~/Downloads/2025 DS Case Study/resume_parser_langchain.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "parser.parse_resume??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing resume...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/resume_parser_langchain.py\", line 479, in parse_resume\n",
      "    result_state = extraction_graph.invoke(initial_state)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1249, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 835, in stream\n",
      "    _panic_or_proceed(done, inflight, step)\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1338, in _panic_or_proceed\n",
      "    raise exc\n",
      "  File \"/Users/siddharth.singh/.local/share/uv/python/cpython-3.11.9-macos-aarch64-none/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 66, in run_with_retry\n",
      "    task.proc.invoke(task.input, task.config)\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2499, in invoke\n",
      "    input = step.invoke(\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/utils.py\", line 89, in invoke\n",
      "    ret = context.run(self.func, input, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/resume_parser_langchain.py\", line 349, in _extract_basic_info\n",
      "    return generic_resume_extraction(state, BasicInfoSchema, 'extract_basic_info', self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddharth.singh/Downloads/2025 DS Case Study/resume_utils.py\", line 256, in generic_resume_extraction\n",
      "    extracted_data = parsed_result.tool_input\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'AgentFinish' object has no attribute 'tool_input'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Resume extraction failed: 'AgentFinish' object has no attribute 'tool_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/resume_parser_langchain.py:479\u001b[39m, in \u001b[36mResumeParser.parse_resume\u001b[39m\u001b[34m(self, file_path)\u001b[39m\n\u001b[32m    478\u001b[39m extraction_graph = \u001b[38;5;28mself\u001b[39m._build_extraction_graph()\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m result_state = \u001b[43mextraction_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Extract structured data from result state (LangGraph returns dict)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1249\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[39m\n\u001b[32m   1248\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1260\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:835\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1338\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(done, inflight, step)\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[32m   1341\u001b[39m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.9-macos-aarch64-none/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py:66\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config)\u001b[39m\n\u001b[32m   2498\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps):\n\u001b[32m-> \u001b[39m\u001b[32m2499\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2500\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2501\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[32m   2502\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2503\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2504\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2506\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/langgraph/utils.py:89\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config)\u001b[39m\n\u001b[32m     84\u001b[39m     kwargs = (\n\u001b[32m     85\u001b[39m         {**\u001b[38;5;28mself\u001b[39m.kwargs, \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: config}\n\u001b[32m     86\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m.func)\n\u001b[32m     87\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.kwargs\n\u001b[32m     88\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/resume_parser_langchain.py:349\u001b[39m, in \u001b[36mResumeParser._extract_basic_info\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Extract basic contact information\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneric_resume_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBasicInfoSchema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mextract_basic_info\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/resume_utils.py:256\u001b[39m, in \u001b[36mgeneric_resume_extraction\u001b[39m\u001b[34m(state, schema_class, function_name, config)\u001b[39m\n\u001b[32m    255\u001b[39m parsed_result = parser.invoke(msg)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m extracted_data = \u001b[43mparsed_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_input\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Create schema instance\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'AgentFinish' object has no attribute 'tool_input'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Process: Run LangGraph workflow\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ”„ Processing resume...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Processing complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/resume_parser_langchain.py:503\u001b[39m, in \u001b[36mResumeParser.parse_resume\u001b[39m\u001b[34m(self, file_path)\u001b[39m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n\u001b[32m    502\u001b[39m     traceback.print_exc()\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResume extraction failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# Step 4: Generate UUIDs for chunks\u001b[39;00m\n\u001b[32m    506\u001b[39m chunked_docs = []\n",
      "\u001b[31mValueError\u001b[39m: Resume extraction failed: 'AgentFinish' object has no attribute 'tool_input'"
     ]
    }
   ],
   "source": [
    "# Process: Run LangGraph workflow\n",
    "print(\"ðŸ”„ Processing resume...\")\n",
    "result = parser.parse_resume(doc_name)\n",
    "print(\"âœ… Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: View Results\n",
    "\n",
    "Structured data extracted from the resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display structured output\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Show full result\n",
    "print(\"=\" * 80)\n",
    "print(\"FULL RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary view\n",
    "print(\"=\" * 80)\n",
    "print(\"QUICK SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "if 'structured_data' in result:\n",
    "    data = result['structured_data']\n",
    "    if hasattr(data, 'model_dump'):\n",
    "        data = data.model_dump()\n",
    "    \n",
    "    print(f\"Name: {data.get('name', 'N/A')}\")\n",
    "    print(f\"Email: {data.get('email', 'N/A')}\")\n",
    "    print(f\"Phone: {data.get('phone', 'N/A')}\")\n",
    "    print(f\"Location: {data.get('location', 'N/A')}\")\n",
    "    print(f\"Experience Years: {data.get('experience_years', 'N/A')}\")\n",
    "    print(f\"Work Experience Entries: {len(data.get('work_experience', []))}\")\n",
    "    print(f\"Education Entries: {len(data.get('education', []))}\")\n",
    "    skills = data.get('skills', {})\n",
    "    if isinstance(skills, dict):\n",
    "        print(f\"Skills: {len(skills.get('technical_skills', []))}\")\n",
    "    else:\n",
    "        print(f\"Skills: N/A\")\n",
    "    \n",
    "print(f\"\\nResume UUID: {result.get('resume_uuid', 'N/A')}\")\n",
    "print(f\"Extraction Timestamp: {result.get('extraction_timestamp', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: Basic Info Extraction\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test script to debug structured output extraction.\n",
    "Tests basic structured output functionality before fixing the main extraction.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "# Simple test schema\n",
    "class BasicInfoSchema(BaseModel):\n",
    "    name: str = Field(description=\"Full name\")\n",
    "    email: str = Field(default=None, description=\"Email address\")\n",
    "    location: str = Field(default=None, description=\"Location\")\n",
    "\n",
    "# Work Experience schema\n",
    "class WorkExperience(BaseModel):\n",
    "    \"\"\"Detailed work experience schema\"\"\"\n",
    "    company: str = Field(default=\"\", description=\"Company name - extract the full company name exactly as written, return empty string if not present\")\n",
    "    position: str = Field(default=\"\", description=\"Job title/position - extract the exact job title including prefixes like 'Senior', 'Lead', etc., return empty string if not present\")\n",
    "    start_date: str = Field(default=\"\", description=\"Start date - extract in any format found (month/year, year only, or full date), return empty string if not present\")\n",
    "    end_date: str = Field(default=\"\", description=\"End date or 'Present' if currently employed - return empty string if not present\")\n",
    "    duration_months: Optional[float] = Field(default=None, description=\"Duration in months - calculate from start and end dates if possible\")\n",
    "    location: str = Field(default=\"\", description=\"Job location - city/country where the job was located, return empty string if not mentioned\")\n",
    "    description: str = Field(default=\"\", description=\"Job responsibilities and achievements - extract ALL bullet points, responsibilities, and accomplishments comprehensively, return empty string if not present\")\n",
    "    sector: str = Field(default=\"\", description=\"Industry sector - identify sector like 'Finance', 'Technology', 'Healthcare', etc., infer from company name or description if needed, return empty string if not present\")\n",
    "\n",
    "class WorkExperienceSchema(BaseModel):\n",
    "    \"\"\"Work experience schema\"\"\"\n",
    "    work_experience: List[WorkExperience] = Field(\n",
    "        description=\"Complete work history with ALL positions including full-time, part-time, internships, contract roles, research positions\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# Education schema\n",
    "class Education(BaseModel):\n",
    "    \"\"\"Education schema\"\"\"\n",
    "    degree: str = Field(default=\"\", description=\"Degree - extract full degree name (Bachelor's, Master's, MBA, PhD, etc.) or abbreviations (BS, BA, MS, MA, MBA, PhD) as written, return empty string if not present\")\n",
    "    field: str = Field(default=\"\", description=\"Field of study - extract major, specialization, or field (e.g., Computer Science, Business Administration, Engineering), return empty string if not present\")\n",
    "    institution: str = Field(default=\"\", description=\"University/Institution name - extract full name exactly as written, return empty string if not present\")\n",
    "    graduation_year: Optional[int] = Field(default=None, description=\"Graduation year - extract year of graduation or completion if mentioned\")\n",
    "    gpa: str = Field(default=\"\", description=\"GPA if mentioned - extract GPA exactly as written (e.g., '3.8/4.0' or '3.8'), return empty string if not present\")\n",
    "    honors: str = Field(default=\"\", description=\"Honors or distinctions - extract honors like 'Summa Cum Laude', 'Dean's List', 'With Distinction', etc., return empty string if not mentioned\")\n",
    "\n",
    "class EducationSchema(BaseModel):\n",
    "    \"\"\"Education schema\"\"\"\n",
    "    education: List[Education] = Field(\n",
    "        description=\"Complete education history\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# Skills schema\n",
    "class SkillsSchema(BaseModel):\n",
    "    \"\"\"Skills and competencies schema\"\"\"\n",
    "    skills: List[str] = Field(description=\"Technical and soft skills - extract ALL skills from entire resume including skills sections, work experience descriptions, and project descriptions - return empty list [] if not present\", default_factory=list)\n",
    "    programming_languages: List[str] = Field(description=\"Programming languages - extract ALL programming/scripting/query languages mentioned anywhere in resume (Python, Java, SQL, R, etc.) - return empty list [] if not present\", default_factory=list)\n",
    "    tools: List[str] = Field(description=\"Tools and software - extract ALL tools, platforms, software, frameworks, databases mentioned anywhere in resume (Excel, AWS, Docker, Git, Bloomberg Terminal, etc.) - return empty list [] if not present\", default_factory=list)\n",
    "\n",
    "# Additional info schema\n",
    "class AdditionalInfoSchema(BaseModel):\n",
    "    \"\"\"Additional information schema\"\"\"\n",
    "    sectors: List[str] = Field(\n",
    "        description=\"Sectors/industries worked in - extract from work experience company names and descriptions, infer from context if needed - return empty list [] if not present\",\n",
    "        default_factory=list\n",
    "    )\n",
    "    languages: List[str] = Field(description=\"Languages spoken - extract ALL languages with proficiency levels if mentioned from entire resume - return empty list [] if not mentioned\", default_factory=list)\n",
    "    publications: List[str] = Field(description=\"Publications if any - extract ALL publications, papers, journal articles from entire resume - return empty list [] if not present\", default_factory=list)\n",
    "    awards: List[str] = Field(description=\"Awards and honors - extract ALL awards, honors, achievements, recognition from entire resume including education and work sections - return empty list [] if not present\", default_factory=list)\n",
    "    others: Dict[str, Any] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Additional structured information not covered by other fields (volunteer work, memberships, patents, etc.) - return empty dict {} if not present\"\n",
    "    )\n",
    "\n",
    "# Initialize model\n",
    "model = ChatOpenAI(\n",
    "    model=\"aws/global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    openai_api_key='',\n",
    "    openai_api_base=\""\n",
    ")\n",
    "\n",
    "# Sample resume text (from Chen Li resume)\n",
    "resume_text = \"\"\"\n",
    "# Chen Li (Alex)\n",
    "\n",
    "Mobile: +852 65478923  \n",
    "Email: Alex_chen2024@gmail.com\n",
    "\n",
    "# EDUCATION\n",
    "\n",
    "## University of Hong Kong Hong Kong\n",
    "Master of Science in Financial Technology Sept.2021 â€“ Jul.2023  \n",
    "Related Courses: Algorithmic Trading, Quantitative Risk Management, Machine Learning in Trading and Finance\n",
    "\n",
    "## University of Hong Kong Hong Kong\n",
    "Bachelor of Business Administration (Finance) Sept.2017 â€“ Jul.2021  \n",
    "Related Courses: Econometrics, Statistics\n",
    "\n",
    "# WORKING EXPERIENCE\n",
    "\n",
    "## Bank of China Hong Kong\n",
    "Investment Analyst June.2021 â€“ Sep.2023\n",
    "\n",
    "Conduct comprehensive quantitative analysis on large-scale Asian equity datasets to provide statistical support for portfolio managers. Analyze market trends, sector rotations, and statistical anomalies, delivering actionable insights for investment decision-making and strategy optimization.\n",
    "\n",
    "Independently developed, constructed and actively managed a multi-factor quantitative model focusing on Greater China technology equities, achieving superior risk-adjusted returns with Sharpe ratio of 1.8+ over 12-month period.\n",
    "\n",
    "Established automated market surveillance system to monitor real-time market conditions, news sentiment, and macro events to identify investment opportunities and manage portfolio risk exposures.\n",
    "\n",
    "Conducted rigorous backtesting and performance attribution analysis of systematic trading strategies, utilizing advanced statistical methods and machine learning techniques to enhance strategy performance and risk management.\n",
    "\n",
    "## Investment Intern Dec.2019 â€“ May.2021\n",
    "\n",
    "Performed comprehensive research on Asian technology and healthcare sectors, covering 15+ sub-industries and 35+ companies across Greater China markets. Supported investment team by conducting expert interviews, analyzing financial statements, building DCF and relative valuation models, and preparing detailed investment research reports for portfolio review.\n",
    "\n",
    "## Chinese University of Hong Kong Hong Kong\n",
    "Research Assistant (Economics Area) Mar.2019 â€“ Mar.2021\n",
    "\n",
    "Efficiently processed and analyzed over 200,000 historical Asian market firm-level data points using Python and R, conducting comprehensive statistical analysis for academic research. Implemented advanced data processing techniques resulting in 25x improvement in research team computational efficiency.\n",
    "\n",
    "Utilized Python and GIS mapping tools to analyze regional economic activity data from satellite imagery, generating 400+ analytical outputs including statistical reports and visualization packages for publication.\n",
    "\n",
    "# Work Experience\n",
    "\n",
    "## Meridian Asia Capital Jiangxi, China\n",
    "\n",
    "**Finance Market Department Intern (Chinese Bond and Equity Market)** Jul.2019 â€“ Aug.2019\n",
    "\n",
    "â€¢ Conducted thorough investigation and analysis of equity information, resulting in the preparation of two comprehensive Equity Pledge Business Investigation Reports.\n",
    "\n",
    "â€¢ Assisted in company research and played a key role in drafting Bill Credit Investigation Reports.\n",
    "\n",
    "# Leadership and Involvement\n",
    "\n",
    "## Fujia Aid Teaching Society Hong Kong\n",
    "\n",
    "**Vice President & Head of Public Relations Team** Sep.2019 â€“ Jun.2020\n",
    "\n",
    "â€¢ Developed strategic initiatives for the organization and supervised two regional programs: Guangzhou Rural Education Project and Shenzhen Migrant Children Support Initiative.\n",
    "\n",
    "â€¢ Led organizational marketing efforts across CUHK campus, increasing membership by 40% and creating educational video content to expand community engagement and awareness.\n",
    "\n",
    "## University Swimming Club Hong Kong\n",
    "\n",
    "**Head of Promotion Team** Sep.2018 â€“ Sep.2019\n",
    "\n",
    "â€¢ Managed promotional activities for the swimming club, collaborating with multiple student organizations to coordinate events including Inter-University Swimming Gala (80+ participants) and Water Safety Workshop series.\n",
    "\n",
    "â€¢ Developed and maintained digital communication platforms including WeChat groups and Instagram accounts to enhance member engagement and information dissemination.\n",
    "\n",
    "# Personal Information\n",
    "\n",
    "**Languages:** English (fluent); Chinese (Mandarin: native, Cantonese: fluent)\n",
    "\n",
    "**Computer Skills:** Python, R, SQL, Microsoft Office Suite (Excel with VBA, PowerPoint, Word), Bloomberg Terminal, WIND Database\n",
    "\n",
    "**Awards:** Dean's List (4 semesters), Outstanding Academic Achievement Award, Top 5% in Provincial University Entrance Examination\n",
    "\n",
    "**Personal Interests:** Swimming, Rock climbing, Violin performance, Documentary filmmaking (Regional Competition Winner)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: Basic Info Extraction\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result dict: {'name': 'John Doe', 'email': 'john@example.com', 'location': 'Singapore'}\n",
      "Schema instance: name='John Doe' email='john@example.com' location='Singapore'\n",
      "\n",
      "================================================================================\n",
      "TEST 2: Work Experience Extraction\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "structured_model_basic = model.with_structured_output(BasicInfoSchema)\n",
    "prompt_basic = \"\"\"Extract the following information from the text below. Extract ALL available fields.\n",
    "\n",
    "Extract:\n",
    "- Full name (REQUIRED)\n",
    "- Email address (if present)\n",
    "- Location (if present)\n",
    "\n",
    "Text to extract from:\n",
    "Name: John Doe, Email: john@example.com, Location: Singapore\n",
    "\"\"\"\n",
    "\n",
    "result_basic = structured_model_basic.invoke([HumanMessage(content=prompt_basic)])\n",
    "print(\"Result dict:\", result_basic)\n",
    "print(\"Schema instance:\", BasicInfoSchema(**result_basic))\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 2: Work Experience Extraction\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Debug: Result Type: <class 'dict'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'work_experience'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDebug: Result Type:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(result_work))\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Accessing the list from the Pydantic object\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m work_entries = \u001b[43mresult_work\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwork_experience\u001b[49m \n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of work experiences extracted:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(work_entries))\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m work_entries:\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'work_experience'"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# 1. Update the instruction to explicitly mention JSON for the proxy's sake\n",
    "prompt_work = f\"\"\"Extract ALL work experience entries from this resume.\n",
    "Return the output as a JSON object matching the schema.\n",
    "\n",
    "Extract EVERY position mentioned, including:\n",
    "- Full-time positions\n",
    "- Part-time positions\n",
    "- Internships (even if labeled as \"Intern\" or \"Internship\")\n",
    "- Contract positions\n",
    "- Research positions\n",
    "- Volunteer work (if it's professional experience)\n",
    "- Consulting roles\n",
    "- Freelance work\n",
    "\n",
    "For EACH position found, extract:\n",
    "- Company name (extract the full company name exactly as written, if not present use empty string '')\n",
    "- Job title/position (extract the exact job title including any prefixes like \"Senior\", \"Junior\", \"Lead\", etc., if not present use empty string '')\n",
    "- Start date (extract start date in any format found - month/year, year only, or full date, if not present use empty string '')\n",
    "- End date (extract end date or \"Present\" if currently employed, if not present use empty string '')\n",
    "- Duration in months (calculate from start and end dates if possible, otherwise leave as null)\n",
    "- Location (city/country where the job was located, if mentioned, otherwise empty string '')\n",
    "- Description (extract ALL responsibilities, achievements, and key accomplishments - be thorough and comprehensive, include all bullet points and details, if not present use empty string '')\n",
    "- Sector/industry (identify the industry sector like \"Finance\", \"Technology\", \"Healthcare\", \"Consulting\", etc., if not explicitly stated infer from company name or description, if not present use empty string '')\n",
    "\n",
    "CRITICAL: Return a list with ALL work positions found. Even if dates are unclear or some fields are missing, include the position with available information. Do not skip any positions. Be thorough and search the entire resume text.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Use 'json_mode' - this is significantly more reliable with custom API gateways\n",
    "structured_model_work = model.with_structured_output(WorkExperienceSchema, method=\"json_mode\")\n",
    "\n",
    "# 3. Invoke\n",
    "result_work = structured_model_work.invoke([\n",
    "    SystemMessage(content=prompt_work),\n",
    "    HumanMessage(content=human_)\n",
    "])\n",
    "\n",
    "# 4. FIXED ACCESS: result_work is an OBJECT, not a dict. \n",
    "# We use result_work.work_experience (dot notation) or result_work.model_dump()\n",
    "result_work = WorkExperienceSchema(**result_work)\n",
    "print(\"-\" * 30)\n",
    "print(\"Debug: Result Type:\", type(result_work))\n",
    "\n",
    "# Accessing the list from the Pydantic object\n",
    "work_entries = result_work.work_experience \n",
    "\n",
    "print(\"Number of work experiences extracted:\", len(work_entries))\n",
    "\n",
    "if work_entries:\n",
    "    for i, exp in enumerate(work_entries, 1):\n",
    "        print(f\"\\nExperience {i}:\")\n",
    "        print(f\"  Company: {exp.company}\")      # Use dot notation\n",
    "        print(f\"  Position: {exp.position}\")    # Use dot notation\n",
    "        print(f\"  Dates: {exp.start_date} - {exp.end_date}\")\n",
    "else:\n",
    "    print(\"WARNING: No work experience extracted!\")\n",
    "\n",
    "\n",
    "\n",
    "if work_entries:\n",
    "    for i, exp in enumerate(work_entries, 1):\n",
    "        print(f\"\\nExperience {i}:\")\n",
    "        print(f\"  Company: {exp.company}\")      # Use dot notation\n",
    "        print(f\"  Position: {exp.position}\")    # Use dot notation\n",
    "        print(f\"  Dates: {exp.start_date} - {exp.end_date}\")\n",
    "else:\n",
    "    print(\"WARNING: No work experience extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_entries = result_work.work_experience \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experience 1:\n",
      "  Company: \n",
      "  Position: \n",
      "  Dates: June 2021 - Sep 2023\n",
      "\n",
      "Experience 2:\n",
      "  Company: \n",
      "  Position: \n",
      "  Dates: Dec 2019 - May 2021\n",
      "\n",
      "Experience 3:\n",
      "  Company: \n",
      "  Position: \n",
      "  Dates: Mar 2019 - Mar 2021\n",
      "\n",
      "Experience 4:\n",
      "  Company: \n",
      "  Position: \n",
      "  Dates: Jul 2019 - Aug 2019\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work_experience': [{'company_name': 'Bank of China Hong Kong',\n",
       "   'job_title': 'Investment Analyst',\n",
       "   'start_date': 'June 2021',\n",
       "   'end_date': 'Sep 2023',\n",
       "   'duration_in_months': 28,\n",
       "   'location': 'Hong Kong',\n",
       "   'description': 'Conduct comprehensive quantitative analysis on large-scale Asian equity datasets to provide statistical support for portfolio managers. Analyze market trends, sector rotations, and statistical anomalies, delivering actionable insights for investment decision-making and strategy optimization. Independently developed, constructed and actively managed a multi-factor quantitative model focusing on Greater China technology equities, achieving superior risk-adjusted returns with Sharpe ratio of 1.8+ over 12-month period. Established automated market surveillance system to monitor real-time market conditions, news sentiment, and macro events to identify investment opportunities and manage portfolio risk exposures. Conducted rigorous backtesting and performance attribution analysis of systematic trading strategies, utilizing advanced statistical methods and machine learning techniques to enhance strategy performance and risk management.',\n",
       "   'sector_industry': 'Finance'},\n",
       "  {'company_name': 'Bank of China Hong Kong',\n",
       "   'job_title': 'Investment Intern',\n",
       "   'start_date': 'Dec 2019',\n",
       "   'end_date': 'May 2021',\n",
       "   'duration_in_months': 18,\n",
       "   'location': 'Hong Kong',\n",
       "   'description': 'Performed comprehensive research on Asian technology and healthcare sectors, covering 15+ sub-industries and 35+ companies across Greater China markets. Supported investment team by conducting expert interviews, analyzing financial statements, building DCF and relative valuation models, and preparing detailed investment research reports for portfolio review.',\n",
       "   'sector_industry': 'Finance'},\n",
       "  {'company_name': 'Chinese University of Hong Kong',\n",
       "   'job_title': 'Research Assistant (Economics Area)',\n",
       "   'start_date': 'Mar 2019',\n",
       "   'end_date': 'Mar 2021',\n",
       "   'duration_in_months': 24,\n",
       "   'location': 'Hong Kong',\n",
       "   'description': 'Efficiently processed and analyzed over 200,000 historical Asian market firm-level data points using Python and R, conducting comprehensive statistical analysis for academic research. Implemented advanced data processing techniques resulting in 25x improvement in research team computational efficiency. Utilized Python and GIS mapping tools to analyze regional economic activity data from satellite imagery, generating 400+ analytical outputs including statistical reports and visualization packages for publication.',\n",
       "   'sector_industry': 'Education/Research'},\n",
       "  {'company_name': 'Meridian Asia Capital',\n",
       "   'job_title': 'Finance Market Department Intern (Chinese Bond and Equity Market)',\n",
       "   'start_date': 'Jul 2019',\n",
       "   'end_date': 'Aug 2019',\n",
       "   'duration_in_months': 2,\n",
       "   'location': 'Jiangxi, China',\n",
       "   'description': 'Conducted thorough investigation and analysis of equity information, resulting in the preparation of two comprehensive Equity Pledge Business Investigation Reports. Assisted in company research and played a key role in drafting Bill Credit Investigation Reports.',\n",
       "   'sector_industry': 'Finance'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WorkExperienceSchema' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# 4. FIXED ACCESS logic\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Convert to dict safely or use dot notation\u001b[39;00m\n\u001b[32m     48\u001b[39m data = WorkExperienceSchema(**result_work)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m experiences = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mwork_experience\u001b[39m\u001b[33m'\u001b[39m, [])\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mResult dict:\u001b[39m\u001b[33m\"\u001b[39m, data)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of work experiences extracted:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(experiences))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/2025 DS Case Study/.venv/lib/python3.11/site-packages/pydantic/main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'WorkExperienceSchema' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import Optional, List\n",
    "\n",
    "# 1. Update the Schema to allow CamelCase (common in LLM outputs)\n",
    "class WorkExperience(BaseModel):\n",
    "    # Allow the model to use 'responsibilities' or 'description'\n",
    "    company: str = Field(default=\"\")\n",
    "    position: str = Field(default=\"\")\n",
    "    start_date: str = Field(default=\"\")\n",
    "    end_date: str = Field(default=\"\")\n",
    "    description: str = Field(default=\"\", alias=\"responsibilities\") # Map 'responsibilities' to 'description'\n",
    "    location: str = Field(default=\"\")\n",
    "    sector: str = Field(default=\"\")\n",
    "    \n",
    "    model_config = ConfigDict(populate_by_name=True)\n",
    "\n",
    "class WorkExperienceSchema(BaseModel):\n",
    "    # Map 'workExperience' to 'work_experience'\n",
    "    work_experience: List[WorkExperience] = Field(default_factory=list, alias=\"workExperience\")\n",
    "    \n",
    "    model_config = ConfigDict(populate_by_name=True)\n",
    "\n",
    "# 2. Update the Prompt to demand EXACT keys\n",
    "prompt_work = \"\"\"Extract ALL work experience entries from this resume.\n",
    "You MUST use these exact JSON keys:\n",
    "- work_experience (the list of positions)\n",
    "- company\n",
    "- position\n",
    "- start_date\n",
    "- end_date\n",
    "- description (include all bullet points here)\n",
    "- location\n",
    "- sector\n",
    "\n",
    "Resume text:\n",
    "{resume_text}\n",
    "\"\"\"\n",
    "\n",
    "# 3. Use the model\n",
    "structured_model_work = model.with_structured_output(WorkExperienceSchema)\n",
    "result_work = structured_model_work.invoke([\n",
    "    SystemMessage(content=\"You are a strict data extractor.\"),\n",
    "    HumanMessage(content=prompt_work)\n",
    "])\n",
    "\n",
    "# 4. FIXED ACCESS logic\n",
    "# Convert to dict safely or use dot notation\n",
    "data = WorkExperienceSchema(**result_work)\n",
    "experiences = data.get('work_experience', [])\n",
    "\n",
    "print(\"Result dict:\", data)\n",
    "print(\"Number of work experiences extracted:\", len(experiences))\n",
    "\n",
    "if experiences:\n",
    "    for i, exp in enumerate(experiences, 1):\n",
    "        print(f\"\\nExperience {i}:\")\n",
    "        print(f\"  Company: {exp.get('company')}\")\n",
    "        print(f\"  Position: {exp.get('position')}\")\n",
    "        print(f\"  Description: {exp.get('description')[:50]}...\") # Show snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result dict: {'workExperience': [{'company': 'Bank of China Hong Kong', 'position': 'Investment Analyst', 'startDate': 'June 2021', 'endDate': 'September 2023', 'responsibilities': ['Conduct comprehensive quantitative analysis on large-scale Asian equity datasets to provide statistical support for portfolio managers. Analyze market trends, sector rotations, and statistical anomalies, delivering actionable insights for investment decision-making and strategy optimization.', 'Independently developed, constructed and actively managed a multi-factor quantitative model focusing on Greater China technology equities, achieving superior risk-adjusted returns with Sharpe ratio of 1.8+ over 12-month period.', 'Established automated market surveillance system to monitor real-time market conditions, news sentiment, and macro events to identify investment opportunities and manage portfolio risk exposures.', 'Conducted rigorous backtesting and performance attribution analysis of systematic trading strategies, utilizing advanced statistical methods and machine learning techniques to enhance strategy performance and risk management.']}, {'company': 'Bank of China Hong Kong', 'position': 'Investment Intern', 'startDate': 'December 2019', 'endDate': 'May 2021', 'responsibilities': ['Performed comprehensive research on Asian technology and healthcare sectors, covering 15+ sub-industries and 35+ companies across Greater China markets. Supported investment team by conducting expert interviews, analyzing financial statements, building DCF and relative valuation models, and preparing detailed investment research reports for portfolio review.']}, {'company': 'Chinese University of Hong Kong', 'position': 'Research Assistant (Economics Area)', 'startDate': 'March 2019', 'endDate': 'March 2021', 'responsibilities': ['Efficiently processed and analyzed over 200,000 historical Asian market firm-level data points using Python and R, conducting comprehensive statistical analysis for academic research. Implemented advanced data processing techniques resulting in 25x improvement in research team computational efficiency.', 'Utilized Python and GIS mapping tools to analyze regional economic activity data from satellite imagery, generating 400+ analytical outputs including statistical reports and visualization packages for publication.']}, {'company': 'Meridian Asia Capital', 'position': 'Finance Market Department Intern (Chinese Bond and Equity Market)', 'startDate': 'July 2019', 'endDate': 'August 2019', 'responsibilities': ['Conducted thorough investigation and analysis of equity information, resulting in the preparation of two comprehensive Equity Pledge Business Investigation Reports.', 'Assisted in company research and played a key role in drafting Bill Credit Investigation Reports.']}]}\n",
      "Number of work experiences extracted: 0\n",
      "WARNING: No work experience extracted!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt_work = \"\"\"You are an expert resume parser. Extract ALL work experience.\n",
    "\n",
    "STRUCTURE RULES:\n",
    "1. The COMPANY NAME is usually found in the headers (marked with ## or in bold). \n",
    "2. The POSITION/TITLE is usually located immediately below the company name or right next to the dates.\n",
    "3. For research roles or internships, treat the University or Laboratory as the Company.\n",
    "\n",
    "CRITICAL: Do not return empty strings for Company or Position if the information exists in the text. Look at the lines immediately preceding the dates.\n",
    "\n",
    "Return the data as a JSON object matching the schema.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "human_ =  f\"Please extract the work experience from the following resume text:\\n\\n{resume_text}\"\n",
    "structured_model_work = model.with_structured_output(WorkExperienceSchema)\n",
    "\n",
    "result_work = structured_model_work.invoke([\n",
    "        SystemMessage(content=prompt_work),\n",
    "        HumanMessage(content=human_)\n",
    "    ])\n",
    "print(\"Result dict:\", result_work)\n",
    "print(\"Number of work experiences extracted:\", len(result_work.get('work_experience', [])))\n",
    "if result_work.get('work_experience'):\n",
    "    for i, exp in enumerate(result_work['work_experience'], 1):\n",
    "        print(f\"\\nExperience {i}:\")\n",
    "        print(f\"  Company: {exp.get('company', 'N/A')}\")\n",
    "        print(f\"  Position: {exp.get('position', 'N/A')}\")\n",
    "        print(f\"  Dates: {exp.get('start_date', 'N/A')} - {exp.get('end_date', 'N/A')}\")\n",
    "else:\n",
    "    print(\"WARNING: No work experience extracted!\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result dict: {'work_experience': [{'company': 'Bank of China', 'position': 'Investment Analyst', 'start_date': 'June 2021', 'end_date': 'September 2023', 'description': 'Conduct comprehensive quantitative analysis on large-scale Asian equity datasets to provide statistical support for portfolio managers. Analyze market trends, sector rotations, and statistical anomalies, delivering actionable insights for investment decision-making and strategy optimization. Independently developed, constructed and actively managed a multi-factor quantitative model focusing on Greater China technology equities, achieving superior risk-adjusted returns with Sharpe ratio of 1.8+ over 12-month period. Established automated market surveillance system to monitor real-time market conditions, news sentiment, and macro events to identify investment opportunities and manage portfolio risk exposures. Conducted rigorous backtesting and performance attribution analysis of systematic trading strategies, utilizing advanced statistical methods and machine learning techniques to enhance strategy performance and risk management.', 'location': 'Hong Kong', 'sector': 'Banking/Investment'}, {'company': 'Bank of China', 'position': 'Investment Intern', 'start_date': 'December 2019', 'end_date': 'May 2021', 'description': 'Performed comprehensive research on Asian technology and healthcare sectors, covering 15+ sub-industries and 35+ companies across Greater China markets. Supported investment team by conducting expert interviews, analyzing financial statements, building DCF and relative valuation models, and preparing detailed investment research reports for portfolio review.', 'location': 'Hong Kong', 'sector': 'Banking/Investment'}, {'company': 'Chinese University of Hong Kong', 'position': 'Research Assistant (Economics Area)', 'start_date': 'March 2019', 'end_date': 'March 2021', 'description': 'Efficiently processed and analyzed over 200,000 historical Asian market firm-level data points using Python and R, conducting comprehensive statistical analysis for academic research. Implemented advanced data processing techniques resulting in 25x improvement in research team computational efficiency. Utilized Python and GIS mapping tools to analyze regional economic activity data from satellite imagery, generating 400+ analytical outputs including statistical reports and visualization packages for publication.', 'location': 'Hong Kong', 'sector': 'Education/Research'}, {'company': 'Meridian Asia Capital', 'position': 'Finance Market Department Intern (Chinese Bond and Equity Market)', 'start_date': 'July 2019', 'end_date': 'August 2019', 'description': 'Conducted thorough investigation and analysis of equity information, resulting in the preparation of two comprehensive Equity Pledge Business Investigation Reports. Assisted in company research and played a key role in drafting Bill Credit Investigation Reports.', 'location': 'Jiangxi, China', 'sector': 'Finance/Investment'}]}\n",
      "Number of work experiences extracted: 4\n",
      "\n",
      "Experience 1:\n",
      "  Company: Bank of China\n",
      "  Position: Investment Analyst\n",
      "  Dates: June 2021 - September 2023\n",
      "\n",
      "Experience 2:\n",
      "  Company: Bank of China\n",
      "  Position: Investment Intern\n",
      "  Dates: December 2019 - May 2021\n",
      "\n",
      "Experience 3:\n",
      "  Company: Chinese University of Hong Kong\n",
      "  Position: Research Assistant (Economics Area)\n",
      "  Dates: March 2019 - March 2021\n",
      "\n",
      "Experience 4:\n",
      "  Company: Meridian Asia Capital\n",
      "  Position: Finance Market Department Intern (Chinese Bond and Equity Market)\n",
      "  Dates: July 2019 - August 2019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "\n",
    "# 1. SIMPLE SCHEMA (Avoid complex aliases for now)\n",
    "class WorkExperience(BaseModel):\n",
    "    company: str = Field(default=\"\")\n",
    "    position: str = Field(default=\"\")\n",
    "    start_date: str = Field(default=\"\")\n",
    "    end_date: str = Field(default=\"\")\n",
    "    description: str = Field(default=\"\")\n",
    "    location: str = Field(default=\"\")\n",
    "    sector: str = Field(default=\"\")\n",
    "\n",
    "class WorkExperienceSchema(BaseModel):\n",
    "    work_experience: List[WorkExperience] = Field(default_factory=list)\n",
    "\n",
    "# 2. THE STRICT PROMPT (Forces the model to use your specific keys)\n",
    "system_prompt = \"\"\"You are a resume parser. Extract ALL work history.\n",
    "You MUST use the following JSON keys exactly as written (SNAKE_CASE):\n",
    "- work_experience\n",
    "- company\n",
    "- position\n",
    "- start_date\n",
    "- end_date\n",
    "- description (Combine all bullet points/responsibilities here)\n",
    "- location\n",
    "- sector\n",
    "\"\"\"\n",
    "\n",
    "# 3. INITIALIZE WITH JSON MODE (Most stable for proxies like GrabGPT)\n",
    "structured_model_work = model.with_structured_output(WorkExperienceSchema, method=\"json_mode\")\n",
    "\n",
    "# 4. RUN\n",
    "result_work = structured_model_work.invoke([\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=f\"Resume text:\\n{resume_text}\")\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Result dict:\", result_work)\n",
    "print(\"Number of work experiences extracted:\", len(result_work.get('work_experience', [])))\n",
    "if result_work.get('work_experience'):\n",
    "    for i, exp in enumerate(result_work['work_experience'], 1):\n",
    "        print(f\"\\nExperience {i}:\")\n",
    "        print(f\"  Company: {exp.get('company', 'N/A')}\")\n",
    "        print(f\"  Position: {exp.get('position', 'N/A')}\")\n",
    "        print(f\"  Dates: {exp.get('start_date', 'N/A')} - {exp.get('end_date', 'N/A')}\")\n",
    "else:\n",
    "    print(\"WARNING: No work experience extracted!\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work_experience': [{'company': 'Bank of China',\n",
       "   'position': 'Investment Analyst',\n",
       "   'start_date': 'June 2021',\n",
       "   'end_date': 'September 2023',\n",
       "   'description': 'Conduct comprehensive quantitative analysis on large-scale Asian equity datasets to provide statistical support for portfolio managers. Analyze market trends, sector rotations, and statistical anomalies, delivering actionable insights for investment decision-making and strategy optimization. Independently developed, constructed and actively managed a multi-factor quantitative model focusing on Greater China technology equities, achieving superior risk-adjusted returns with Sharpe ratio of 1.8+ over 12-month period. Established automated market surveillance system to monitor real-time market conditions, news sentiment, and macro events to identify investment opportunities and manage portfolio risk exposures. Conducted rigorous backtesting and performance attribution analysis of systematic trading strategies, utilizing advanced statistical methods and machine learning techniques to enhance strategy performance and risk management.',\n",
       "   'location': 'Hong Kong',\n",
       "   'sector': 'Banking/Financial Services'},\n",
       "  {'company': 'Bank of China',\n",
       "   'position': 'Investment Intern',\n",
       "   'start_date': 'December 2019',\n",
       "   'end_date': 'May 2021',\n",
       "   'description': 'Performed comprehensive research on Asian technology and healthcare sectors, covering 15+ sub-industries and 35+ companies across Greater China markets. Supported investment team by conducting expert interviews, analyzing financial statements, building DCF and relative valuation models, and preparing detailed investment research reports for portfolio review.',\n",
       "   'location': 'Hong Kong',\n",
       "   'sector': 'Banking/Financial Services'},\n",
       "  {'company': 'Chinese University of Hong Kong',\n",
       "   'position': 'Research Assistant (Economics Area)',\n",
       "   'start_date': 'March 2019',\n",
       "   'end_date': 'March 2021',\n",
       "   'description': 'Efficiently processed and analyzed over 200,000 historical Asian market firm-level data points using Python and R, conducting comprehensive statistical analysis for academic research. Implemented advanced data processing techniques resulting in 25x improvement in research team computational efficiency. Utilized Python and GIS mapping tools to analyze regional economic activity data from satellite imagery, generating 400+ analytical outputs including statistical reports and visualization packages for publication.',\n",
       "   'location': 'Hong Kong',\n",
       "   'sector': 'Education/Research'},\n",
       "  {'company': 'Meridian Asia Capital',\n",
       "   'position': 'Finance Market Department Intern (Chinese Bond and Equity Market)',\n",
       "   'start_date': 'July 2019',\n",
       "   'end_date': 'August 2019',\n",
       "   'description': 'Conducted thorough investigation and analysis of equity information, resulting in the preparation of two comprehensive Equity Pledge Business Investigation Reports. Assisted in company research and played a key role in drafting Bill Credit Investigation Reports.',\n",
       "   'location': 'Jiangxi, China',\n",
       "   'sector': 'Financial Services/Asset Management'}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 3: Education Extraction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "structured_model_edu = model.with_structured_output(EducationSchema)\n",
    "prompt_edu = f\"\"\"Extract ALL education entries from this resume.\n",
    "\n",
    "IMPORTANT: Look for education in sections titled: \"Education\", \"Academic Background\", \"Qualifications\", \"Degrees\", or similar. Extract EVERY degree, certification, or educational qualification mentioned, including undergraduate, graduate, and any additional degrees.\n",
    "\n",
    "For EACH education entry found, extract:\n",
    "- Degree (extract the full degree name: Bachelor's, Master's, MBA, PhD, Associate's, Diploma, Certificate, etc. Use abbreviations like BS, BA, MS, MA, MBA, PhD if that's what's written, if not present use empty string '')\n",
    "- Field of study (extract the major, specialization, or field: Computer Science, Business Administration, Engineering, etc., if not present use empty string '')\n",
    "- Institution name (extract the full university, college, or institution name, if not present use empty string '')\n",
    "- Graduation year (extract the year of graduation or completion, if mentioned)\n",
    "- GPA (extract GPA if mentioned (e.g., \"3.8/4.0\" or \"3.8\"), otherwise empty string '')\n",
    "- Honors (extract any honors, distinctions, or awards: \"Summa Cum Laude\", \"Dean's List\", \"With Distinction\", etc., if mentioned, otherwise empty string '')\n",
    "\n",
    "CRITICAL: Return a list with ALL education entries found. Include undergraduate, graduate, and any additional degrees or certifications. Even if some fields are missing, include the entry with available information.\n",
    "\n",
    "Resume text:\n",
    "{resume_text}\n",
    "\"\"\"\n",
    "\n",
    "result_edu = structured_model_edu.invoke([HumanMessage(content=prompt_edu)])\n",
    "print(\"Result dict:\", result_edu)\n",
    "print(\"Number of education entries extracted:\", len(result_edu.get('education', [])))\n",
    "if result_edu.get('education'):\n",
    "    for i, edu in enumerate(result_edu['education'], 1):\n",
    "        print(f\"\\nEducation {i}:\")\n",
    "        print(f\"  Degree: {edu.get('degree', 'N/A')}\")\n",
    "        print(f\"  Field: {edu.get('field', 'N/A')}\")\n",
    "        print(f\"  Institution: {edu.get('institution', 'N/A')}\")\n",
    "        print(f\"  Year: {edu.get('graduation_year', 'N/A')}\")\n",
    "else:\n",
    "    print(\"WARNING: No education extracted!\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 4: Skills Extraction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "structured_model_skills = model.with_structured_output(SkillsSchema)\n",
    "prompt_skills = f\"\"\"Extract ALL skills from this resume.\n",
    "\n",
    "IMPORTANT: Search the ENTIRE resume comprehensively. Look for skills in sections titled: \"Skills\", \"Technical Skills\", \"Core Competencies\", \"Proficiencies\", \"Expertise\", \"Computer Skills\", \"Technical Expertise\", \"Tools & Technologies\", or ANY section that lists skills. Also extract skills mentioned in work experience descriptions, project descriptions, education sections, and anywhere else in the resume.\n",
    "\n",
    "Extract into three categories:\n",
    "\n",
    "1. Technical and soft skills (list):\n",
    "   - Include technical skills: data analysis, machine learning, project management, statistical analysis, quantitative analysis, etc.\n",
    "   - Include soft skills: leadership, communication, teamwork, problem-solving, strategic thinking, etc.\n",
    "   - Include domain-specific skills: financial modeling, risk management, software development, algorithmic trading, etc.\n",
    "   - Extract ALL skills mentioned, even if they appear in different sections\n",
    "   - Look for skills mentioned in bullet points, job descriptions, and project descriptions\n",
    "\n",
    "2. Programming languages (list):\n",
    "   - Extract ALL programming languages: Python, Java, C++, JavaScript, SQL, R, MATLAB, VBA, etc.\n",
    "   - Include scripting languages (Python, Bash, PowerShell), query languages (SQL), and markup languages (HTML, XML) if relevant\n",
    "   - Be thorough - extract from skills sections AND work experience descriptions AND project descriptions\n",
    "   - Look for mentions like \"Python\", \"R\", \"SQL\", \"Java\", etc. in any context\n",
    "\n",
    "3. Tools and software (list):\n",
    "   - Extract ALL tools, platforms, and software mentioned anywhere in the resume\n",
    "   - Examples: Excel, Tableau, AWS, Docker, Git, JIRA, Salesforce, Bloomberg Terminal, WIND Database, Microsoft Office Suite, etc.\n",
    "   - Include frameworks, libraries, databases, cloud platforms, development tools, financial tools, etc.\n",
    "   - Extract from both explicit skills lists AND work experience descriptions AND project descriptions\n",
    "   - Look for tool names in job responsibilities, achievements, and technical sections\n",
    "   - Include database systems (MySQL, PostgreSQL, MongoDB), cloud platforms (AWS, Azure, GCP), version control (Git, SVN), etc.\n",
    "\n",
    "CRITICAL: Be comprehensive and thorough. Extract skills from:\n",
    "- Dedicated skills sections (any format)\n",
    "- Work experience job descriptions (read all bullet points carefully)\n",
    "- Project descriptions\n",
    "- Education sections (courses, coursework)\n",
    "- Certifications sections\n",
    "- Anywhere else skills, languages, or tools are mentioned\n",
    "\n",
    "Return empty lists [] only if NO skills are found anywhere in the resume. When in doubt, include it.\n",
    "\n",
    "Resume text:\n",
    "{resume_text}\n",
    "\"\"\"\n",
    "\n",
    "result_skills = structured_model_skills.invoke([HumanMessage(content=prompt_skills)])\n",
    "print(\"Result dict:\", result_skills)\n",
    "print(f\"Skills extracted: {len(result_skills.get('skills', []))} items\")\n",
    "if result_skills.get('skills'):\n",
    "    print(\"  Skills:\", result_skills['skills'][:10])  # Show first 10\n",
    "else:\n",
    "    print(\"  WARNING: No skills extracted!\")\n",
    "\n",
    "print(f\"\\nProgramming languages extracted: {len(result_skills.get('programming_languages', []))} items\")\n",
    "if result_skills.get('programming_languages'):\n",
    "    print(\"  Languages:\", result_skills['programming_languages'])\n",
    "else:\n",
    "    print(\"  WARNING: No programming languages extracted!\")\n",
    "\n",
    "print(f\"\\nTools extracted: {len(result_skills.get('tools', []))} items\")\n",
    "if result_skills.get('tools'):\n",
    "    print(\"  Tools:\", result_skills['tools'][:10])  # Show first 10\n",
    "else:\n",
    "    print(\"  WARNING: No tools extracted!\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 5: Additional Info Extraction (Languages, Awards, etc.)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "structured_model_add = model.with_structured_output(AdditionalInfoSchema)\n",
    "prompt_add = f\"\"\"Extract additional information from this resume.\n",
    "\n",
    "IMPORTANT: Search the ENTIRE resume carefully and comprehensively for this information. Look in dedicated sections, work experience descriptions, education sections, and anywhere else information might be mentioned.\n",
    "\n",
    "Extract:\n",
    "\n",
    "1. Sectors/industries worked in (list):\n",
    "   - Extract from work experience company names and descriptions\n",
    "   - Common sectors: Finance, Technology, Healthcare, Consulting, Manufacturing, Retail, Education, Government, Non-profit, etc.\n",
    "   - If not explicitly listed, infer from company names, job descriptions, and industry context\n",
    "   - Look for sector mentions in company descriptions, job titles, or responsibilities\n",
    "   - Return empty list [] only if no sector information can be determined from anywhere in the resume\n",
    "\n",
    "2. Languages spoken (if mentioned, list):\n",
    "   - Look for sections like \"Languages\", \"Language Skills\", \"Language Proficiency\", \"Languages Spoken\", or language mentions in other sections\n",
    "   - Also check \"Personal Information\" sections or similar areas\n",
    "   - Extract language names and proficiency levels if mentioned (e.g., \"English (Native)\", \"Spanish (Fluent)\", \"Chinese (Mandarin: native, Cantonese: fluent)\")\n",
    "   - Include all languages mentioned with their proficiency levels\n",
    "   - Return empty list [] if not mentioned anywhere\n",
    "\n",
    "3. Publications (if any, list):\n",
    "   - Look for sections like \"Publications\", \"Research\", \"Papers\", \"Published Works\", \"Research Publications\", or similar\n",
    "   - Extract paper titles, journal names, conference names, or publication details\n",
    "   - Include research papers, journal articles, conference papers, book chapters, etc.\n",
    "   - Return empty list [] if not present\n",
    "\n",
    "4. Awards and honors (if any, list):\n",
    "   - Look for sections like \"Awards\", \"Honors\", \"Achievements\", \"Recognition\", \"Accolades\", \"Distinctions\", or similar\n",
    "   - Extract award names, recognition titles, achievement descriptions, competition wins, etc.\n",
    "   - Also check education section for academic honors (Dean's List, Summa Cum Laude, etc.)\n",
    "   - Check work experience for professional awards and recognition\n",
    "   - Include all types of awards: academic, professional, competition wins, scholarships, etc.\n",
    "   - Return empty list [] if not present\n",
    "\n",
    "5. Any other relevant information (dict):\n",
    "   - Extract any other structured information not covered above\n",
    "   - Examples: volunteer work, professional memberships, patents, licenses, hobbies/interests (if professional), leadership roles outside work, etc.\n",
    "   - Look for sections like \"Volunteer Work\", \"Professional Memberships\", \"Patents\", \"Interests\", \"Activities\", etc.\n",
    "   - Format as key-value pairs (e.g., {{\"volunteer_work\": \"...\", \"memberships\": \"...\", \"interests\": \"...\"}})\n",
    "   - Return empty dict {{}} if not present\n",
    "\n",
    "CRITICAL: Be thorough and comprehensive. Search the entire resume text from beginning to end. For list fields, return an empty list [] if no data is present. For dict fields, return an empty dict {{}} if no data is present. Do not return empty strings for list or dict fields. Extract all available information.\n",
    "\n",
    "Resume text:\n",
    "{resume_text}\n",
    "\"\"\"\n",
    "\n",
    "result_add = structured_model_add.invoke([HumanMessage(content=prompt_add)])\n",
    "print(\"Result dict:\", result_add)\n",
    "print(f\"Sectors extracted: {len(result_add.get('sectors', []))} items\")\n",
    "if result_add.get('sectors'):\n",
    "    print(\"  Sectors:\", result_add['sectors'])\n",
    "else:\n",
    "    print(\"  WARNING: No sectors extracted!\")\n",
    "\n",
    "print(f\"\\nLanguages extracted: {len(result_add.get('languages', []))} items\")\n",
    "if result_add.get('languages'):\n",
    "    print(\"  Languages:\", result_add['languages'])\n",
    "else:\n",
    "    print(\"  WARNING: No languages extracted!\")\n",
    "\n",
    "print(f\"\\nAwards extracted: {len(result_add.get('awards', []))} items\")\n",
    "if result_add.get('awards'):\n",
    "    print(\"  Awards:\", result_add['awards'])\n",
    "else:\n",
    "    print(\"  WARNING: No awards extracted!\")\n",
    "\n",
    "print(f\"\\nOthers extracted: {len(result_add.get('others', {}))} keys\")\n",
    "if result_add.get('others'):\n",
    "    print(\"  Others:\", result_add['others'])\n",
    "else:\n",
    "    print(\"  Others: {{}} (empty dict)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL TESTS COMPLETED\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
